xtr <- matrix(rnorm(100*100),ncol=100);#
xte <- matrix(rnorm(100000*100),ncol=100);#
beta <- c(rep(1,10),rep(0,90));#
ytr <- xtr%*%beta + rnorm(100);#
yte <- xte%*%beta + rnorm(100000);#
rsq <- trainerr <- testerr <- NULL;#
#
for(i in 2:100){#
  mod <- lm(ytr~xtr[,1:i])#
  rsq <- c(rsq,summary(mod)$r.squared)#
  beta <- mod$coef[-1]#
  intercept <- mod$coef[1]#
  trainerr <- c(trainerr, mean((xtr[,1:i]%*%beta+intercept - ytr)^2))#
  testerr <- c(testerr, mean((xte[,1:i]%*%beta+intercept - yte)^2))#
}#
#
par(mfrow=c(1,3));#
#
plot(2:100,rsq, xlab="Number of Variables", ylab="R Squared", log="y");#
#
abline(v=10,col="red");#
#
plot(2:100,trainerr, xlab="Number of Variables", ylab="Training Error",log="y");#
#
abline(v=10,col="red");#
#
plot(2:100,testerr, xlab="Number of Variables", ylab="Test Error",log="y");#
#
abline(v=10,col="red")#
#
par(mfrow=c(1,1))
?daisy
install.packages("cluster")
?daisy
library(cluster)
?daisy
?qt
qt(0.975,7)
qt(0.995,7)
###################################################################################### # PART 1 ##############################################################################
# Read in data unlogged = c(22,18,22,20,15,21,13,13,19,13,15,14,19,20,21,24,21,18,14) logged = c(17,4,18,14,18,15,14,11,13,16,11,10,8,9,17)
#### Obtain a side by side box plot boxplot(logged,unlogged, main = "Number of tree species per rain forest plot in Borneo:          Logged vs Unlogged ", col = "purple")
?pnorm
?pnorm(2.5,2.7,0.2)
pnorm(2.5,2.7,0.2)
pnorm(3,2.7,0.2)
0.933-0.159
#setwd("Users/wzhang/MS final project/R functions for VVV, with errors")#
### manual EM algorithm for VVV with estimation error#
#
mcmeVVV = function(data, z, err, errstr="none", d=1, itmax=Inf, lb=1e-3){#
  # Argument "errstr" is user-specified error structure.#
  # If errstr="identical", all errors are the same.#
  # If errstr="cluster", errors are the same within each cluster.#
  # If errstr="none", no constraints on error structure.#
  # When errstr="none", "d" denotes the diagonal element, default = 1#
  # Argument "lb" sets lower bound for diagonal elements of decomposed cov#
  # matrices. Default set at 1e-3=0.001. Can be set to a larger number to#
  # avoid singularity issues.#
  source("E-step_VVV_error.R")#
  source("M-step_VVV_error.R")#
  source("log likelihood_VVV_error.R")#
  source("objective function_VVV_error.R")#
  source("W_k matrix.R")#
  source("inipar.R")#
#
  ## debug(obj.fun.VVV.err);#
  n = nrow(data)#
  p = ncol(data)#
  G = ncol(z)#
  member = z # matrix for storing membership estimates#
  center = matrix(0,p,G) # matrix for storing mean estimates#
#
  k = 2 # keeps track of number of iteration#
#
  piconst = n*p*log(2*pi)/2#
  loglikelihood = NA#
  parameters = list()#
  zhat = matrix(0,n,G)#
#
  llike = rep(0, 1000) # set convergence criterion#
  FLMAX = 1.7976931348623157e308#
  llike[1] = FLMAX/2#
  llike[2] = FLMAX#
  tol = 1e-5#
  # while loop for iteration:#
  repeat{#
    print(paste("iteration =",k-1)) # prints number of iterations#
     # initial values for M-step#
    if(errstr=="identical"){#
      ini.par = ini.par.iderr(data, z, err)#
    } else if(errstr=="cluster"){#
      ini.par = ini.par.clust(data, z, err)#
    } else if(errstr=="none"){#
      ini.par = ini.par.no(data, z, d)#
    }#
    thetahat = MstepVVV.err(z, data, err, ini.par, lb) # M-step#
    temp = EstepVVV.err(thetahat, data, err) # E-step#
    zhat = temp[[1]] # membership estimates#
    parameters = temp[[2]] # parameter estimates#
    loglikelihood = temp[[3]] # records log likelihood#
    z = zhat # update membership matrix#
    member = rbind(member,zhat) # store membership estimates#
    center = rbind(center,parameters$muhat) # store mean estimates#
    llike[k+1] = loglikelihood # update log likelihood of observed data#
    #print(paste("loglik =",loglikelihood))#
    k = k+1 # increment k#
    delta = abs(llike[k-1]-llike[k])/(1+abs(llike[k]))#
    it = k-2#
    if(delta<tol || it>=itmax) break;#
  }#
  error = llike[k]-llike[k-1]#
  uncertainty = numeric() # records classification uncertainty of each obs.#
  for(i in 1:n){#
    rowmax = max(zhat[i,])#
    uncertainty[i] = 1-rowmax#
  }#
  # edit output so it's basically consistent with meVVV() from MCLUST:#
  out = list(modelname="VVV with est error", n=n, d=p, G=G, z=zhat, parameters=parameters, uncertainty=uncertainty,#
    loglik=loglikelihood, iteration=k-2, error=error, member=member, center=center, likvec=llike[1:k][-(1:2)])#
  return(out)#
}
library(mclust)#
library(gdata)#
library(MASS)#
library(phyclust)#
#
col.blue = rgb(30,144,255,max=255)
sim1 = function(seed,n){#
#
  set.seed(seed)#
  n = n#
#
## Simulate two clusters that are clearly separated but close to each other.#
  mu1 = c(-3,0)#
  mu2 = c(3,0)#
  # 3.2 is chosen as square root of chi(0.01) with df=2, so that 99% of data are expected#
  # to fall into the region#
  sig = matrix(c(1,0,0,1),nrow=2)#
  dat1 = mvrnorm(n, mu1, sig)#
  dat2 = mvrnorm(n, mu2, sig)#
#
  #plot(dat1, xlim = c(-7,7), ylim = c(-5,5), col="blue")#
  #points(dat2, col="red")#
  # Observe that the two clusters are separated but close.#
  # Now we truncate each group and keep the observations with x-coord <=-3 and >=3:#
  dat1.tr = dat1[which(dat1[,1]<=-3),]#
  dat2.tr = dat2[which(dat2[,1]>=3),]#
  #plot(dat1.tr, xlim=c(-7,7), ylim=c(-5,5), col="blue")#
  #points(dat2.tr, col="red")#
  #abline(v=3, lty="dashed")#
  #abline(v=-3, lty="dashed")#
  # Define an error matrix:#
  err = matrix(c(4,0,0,4),nrow=2)#
  sig.new = sig + err#
  dat1.new = mvrnorm(n, mu1, sig.new)#
  dat2.new = mvrnorm(n, mu2, sig.new)#
  dat1.new.tr = dat1.new[which(dat1.new[,1]>-3),]#
  dat2.new.tr = dat2.new[which(dat2.new[,1]<3),]#
  group1 = rbind(dat1.tr, dat1.new.tr)#
  group2 = rbind(dat2.tr, dat2.new.tr)#
  n1 = nrow(group1)#
  n2 = nrow(group2)#
  # Combine two groups:#
  dat = rbind(group1, group2)#
  s = nrow(dat)#
  xlow = min(dat[,1])-0.5#
  xup = max(dat[,1])+0.5#
  ylow = min(dat[,2])-0.2#
  yup = max(dat[,2])+0.2#
  #par(mfrow=c(1,3))#
  plot(group1, xlim=c(xlow,xup), ylim=c(ylow,yup), pch=19, main="Generated clusters"#
    ,xlab="",ylab="",col=col.blue,xaxt="n",yaxt="n")#
  points(group2, pch=0, col="red")#
  abline(v=3, lty="dashed")#
  abline(v=-3, lty="dashed")#
  # The combined dataset has points in the middle are commingled.#
  #-----------------------------------------------------------------------------##
  # Construct initial classification:#
  z.ini = matrix(c(rep(c(1,0),n1),rep(c(0,1),n2)),byrow=T, nrow=s)#
  # Construct the true error matrices:#
  temp = array(0,dim=c(2,3,s))#
  for(i in 1:s){#
    temp[,1,i] = dat[i,]#
  }#
  for(i in 1:s){#
    if(temp[1,1,i]>-3 && temp[1,1,i]<3){#
      temp[1,2,i] = temp[2,3,i] = 4#
    }#
  }#
  errmat = array(0,dim=c(2,2,s))#
  for(i in 1:s){#
    errmat[,,i] = temp[,2:3,i]#
  }#
  #-----------------------------------------------------------------------------##
  ## Use meVVV:#
  res1 = meVVV(dat, z.ini)#
  z1 = res1$z#
  t = nrow(z1)#
  G = ncol(z1)#
  for(i in 1:t){#
    rowmax = max(z1[i,])#
    for(k in 1:G){#
      z1[i,k] = ifelse(z1[i,k]==rowmax,2,1)#
    }  #
  }#
  dat.new = cbind(dat,z1)#
  n1 = nrow(group1)#
  group1.new1 = cbind(group1,z1[1:n1,])#
  group2.new1 = cbind(group2,z1[(n1+1):s,])#
  plot(group1, xlim=c(xlow,xup), ylim=c(ylow,yup), pch=ifelse(group1.new1[,3]==2,19,0)#
    ,col=ifelse(group1.new1[,3]==2,col.blue,"red"), xlab="", ylab="",main="meVVV",xaxt="n",yaxt="n")#
  points(group2, pch=ifelse(group2.new1[,3]==2,19,0), col=ifelse(group2.new1[,3]==2,col.blue,"red"))#
  abline(v=3, lty="dashed")#
  abline(v=-3, lty="dashed")#
  #--------------------------------------------------------------------------##
  ## Use MCME:#
  # Perform our clustering method:#
  my.result = mcmeVVV(dat, z.ini, errmat)#
  names(my.result)#
  z2 = my.result$z#
  t = nrow(z2)#
  G = ncol(z2)#
  for(i in 1:t){#
    rowmax = max(z2[i,])#
    for(k in 1:G){#
      z2[i,k] = ifelse(z2[i,k]==rowmax,2,1)#
    }  #
  }#
  dat.new = cbind(dat,z2)#
  n1 = nrow(group1)#
  group1.new2 = cbind(group1,z2[1:n1,])#
  group2.new2 = cbind(group2,z2[(n1+1):s,])#
  plot(group1, xlim=c(xlow,xup), ylim=c(ylow,yup), pch=ifelse(group1.new2[,3]==2,19,0)#
    ,col=ifelse(group1.new2[,3]==2,col.blue,"red"), xlab="", ylab="",main="MCME",xaxt="n",yaxt="n")#
  points(group2, pch=ifelse(group2.new2[,3]==2,19,0), col=ifelse(group2.new2[,3]==2,col.blue,"red"))#
  abline(v=3, lty="dashed")#
  abline(v=-3, lty="dashed")#
  #par(mfrow=c(1,1))#
  #-------------------------------------------------------------------------------------##
  ## Compute Rand index#
  z.ini = z.ini + 1#
  rand1 = RRand(z.ini,z1)#
  rand2 = RRand(z.ini,z2)#
  z.ini = z.ini - 1#
  #-----------------------------------------------------------------------##
  ## Misclassification rate#
  z.ini = z.ini + 1#
  tab1 = table(z.ini,z1)#
  tab2 = table(z.ini,z2)#
  mis1 = 1-sum(diag(tab1))/sum(tab1)#
  mis2 = 1-sum(diag(tab2))/sum(tab2)#
  z.ini = z.ini - 1#
}
png("fifty.png",height=640,width=480,res=300)#
par(mfrow=c(5,3))#
sim1(91,50)#
sim1(92,50)#
sim1(93,50)#
sim1(94,50)#
sim1(95,50)#
par(mfrow=c(1,1))#
dev.off()
png("fifty.png",res=300)#
par(mfrow=c(5,3))#
sim1(91,50)#
sim1(92,50)#
sim1(93,50)#
sim1(94,50)#
sim1(95,50)#
par(mfrow=c(1,1))#
dev.off()
png("fifty.png",height=4,width=4,units="in",res=300)#
par(mfrow=c(5,3))#
sim1(91,50)#
sim1(92,50)#
sim1(93,50)#
sim1(94,50)#
sim1(95,50)#
par(mfrow=c(1,1))#
dev.off()
png("fifty.png",height=6,width=6,units="in",res=300)#
par(mfrow=c(5,3))#
sim1(91,50)#
sim1(92,50)#
sim1(93,50)#
sim1(94,50)#
sim1(95,50)#
par(mfrow=c(1,1))#
dev.off()
png("fifty.png",800,600,res=300)#
par(mfrow=c(5,3))#
sim1(91,50)#
sim1(92,50)#
sim1(93,50)#
sim1(94,50)#
sim1(95,50)#
par(mfrow=c(1,1))#
dev.off()
png("fifty.png",1500,900,res=300)#
par(mfrow=c(5,3))#
sim1(91,50)#
sim1(92,50)#
sim1(93,50)#
sim1(94,50)#
sim1(95,50)#
par(mfrow=c(1,1))#
dev.off()
png("fifty.png",1500,900)#
par(mfrow=c(5,3))#
sim1(91,50)#
sim1(92,50)#
sim1(93,50)#
sim1(94,50)#
sim1(95,50)#
par(mfrow=c(1,1))#
dev.off()
png("fifty.png",1500,900)#
par(mfrow=c(5,3))#
sim1(91,50)#
sim1(92,50)#
sim1(93,50)#
sim1(94,50)#
sim1(95,50)#
par(mfrow=c(1,1))#
dev.off()
?ggsave
library(ggplot2)
